# Task 06 â€” L2 Regularization

## 1. Objective
Compare model performance with different L2 regularization strengths (0.0001, 0.001, 0.01) to understand how weight penalty affects model generalization.

## 2. Code Used
```python
l2_values = [0.0001, 0.001, 0.01]

for l2v in l2_values:
    model = models.Sequential([
        layers.Flatten(input_shape=(28, 28)),
        layers.Dense(128, activation="relu",
                     kernel_regularizer=regularizers.l2(l2v)),
        layers.Dense(10, activation="softmax")
    ])
    model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
    model.fit(x_train, y_train, epochs=15, validation_data=(x_val, y_val), verbose=0)
```

## 3. Results
Models were trained with L2 regularization values of 0.0001, 0.001, and 0.01. The validation loss and accuracy were recorded for each configuration to compare the effect of different regularization strengths.

## 4. Short Analysis
L2 regularization adds a penalty term to the loss function proportional to the sum of squared weights, encouraging smaller weights and preventing **overfitting**. A small L2 value (0.0001) provides mild regularization, while larger values (0.001, 0.01) impose stronger constraints. The **optimizer behavior** (Adam) adjusts weight updates to balance the data loss and the regularization penalty. Too much L2 regularization can cause **underfitting** by constraining the model too much, while too little may not prevent overfitting. The **ReLU activation** combined with L2 regularization helps maintain sparse, efficient weight patterns. This technique improves **generalization** by reducing model complexity.

## 5. Key Takeaway
L2 regularization prevents overfitting by penalizing large weights, but the regularization strength must be carefully tunedâ€”too weak allows overfitting, too strong causes underfitting.

---

## ğŸ—£ï¸ Ø´Ø±Ø­ Ø¨Ø§Ù„Ø¹Ø§Ù…ÙŠØ©

### Ø¥ÙŠÙ‡ Ù‡Ùˆ Ø§Ù„Ù€ L2 RegularizationØŸ
ØªØ®ÙŠÙ„ Ø¥Ù†Ùƒ Ø¨ØªØ¹Ø§Ù‚Ø¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù„Ùˆ Ø§Ù„Ù€ weights Ø¨ØªØ§Ø¹ØªÙ‡ ÙƒØ¨ÙŠØ±Ø© Ù‚ÙˆÙŠ. Ø²ÙŠ Ù…Ø§ ØªÙ‚ÙˆÙ„Ù‡ "Ø®Ù„ÙŠÙƒ Ø¨Ø³ÙŠØ·!"

### Ø¥Ø²Ø§ÙŠ Ø¨ÙŠØ´ØªØºÙ„ØŸ
- Ø¨Ù†Ø¶ÙŠÙ Ø¹Ù‚ÙˆØ¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù€ loss Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø­Ø¬Ù… Ø§Ù„Ù€ weights
- ÙƒÙ„ Ù…Ø§ Ø§Ù„Ù€ weights Ø£ÙƒØ¨Ø±ØŒ Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø© Ø£ÙƒØ¨Ø±
- Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨ÙŠØ­Ø§ÙˆÙ„ ÙŠØ®Ù„ÙŠ Ø§Ù„Ù€ weights ØµØºÙŠØ±Ø©

### Ù„ÙŠÙ‡ Ø¯Ù‡ Ù…ÙÙŠØ¯ØŸ
- Ø§Ù„Ù€ weights Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¨ØªØ®Ù„ÙŠ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ "ÙŠØ­ÙØ¸" Ø§Ù„ØªØ¯Ø±ÙŠØ¨
- Ù„Ù…Ø§ Ù†Ø®Ù„ÙŠÙ‡Ø§ ØµØºÙŠØ±Ø©ØŒ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨ÙŠØªØ¹Ù„Ù… patterns Ø¹Ø§Ù…Ø©
- Ø¨ÙŠØ­Ø³Ù† Ø§Ù„Ù€ generalization

### Ø¥ÙŠÙ‡ Ø§Ù„Ù„ÙŠ Ø¬Ø±Ø¨Ù†Ø§Ù‡ØŸ
- 0.0001 - Ø¹Ù‚ÙˆØ¨Ø© Ø®ÙÙŠÙØ©
- 0.001 - Ø¹Ù‚ÙˆØ¨Ø© Ù…ØªÙˆØ³Ø·Ø©
- 0.01 - Ø¹Ù‚ÙˆØ¨Ø© Ù‚ÙˆÙŠØ©

### Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† L2 Ùˆ DropoutØŸ
- **Dropout**: Ø¨ÙŠØ·ÙÙŠ neurons Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹
- **L2**: Ø¨ÙŠØ®Ù„ÙŠ Ø§Ù„Ù€ weights ØµØºÙŠØ±Ø©

### Ø§Ù„Ø®Ù„Ø§ØµØ©
Ø§Ù„Ù€ L2 Ø²ÙŠ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ø§Ù„Ù„ÙŠ Ø¨ÙŠÙ‚ÙˆÙ„Ùƒ "Ù…ØªØ¹Ù‚Ø¯Ø´ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹!" - Ø¨ÙŠØ®Ù„ÙŠ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ø³ÙŠØ· ÙˆÙØ¹Ø§Ù„. âš¡
