# Task 07 â€” Optimizers

## 1. Objective
Compare the performance of different optimization algorithms (SGD, Momentum, Adam, AdamW) to understand how optimizer choice affects training dynamics and convergence.

## 2. Code Used
```python
optimizers = {
    "SGD": tf.keras.optimizers.SGD(0.01),
    "Momentum": tf.keras.optimizers.SGD(0.01, momentum=0.9),
    "Adam": tf.keras.optimizers.Adam(),
    "AdamW": tf.keras.optimizers.AdamW()
}

for name, opt in optimizers.items():
    model = models.clone_model(base_model)
    model.compile(optimizer=opt, loss="sparse_categorical_crossentropy", metrics=["accuracy"])
    history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val), verbose=0)
```

## 3. Results
Validation loss curves for SGD, Momentum, Adam, and AdamW optimizers are plotted and saved to `results/optimizer_tests/optimizer_comparison.png`. The curves show convergence speed and final performance for each optimizer.

## 4. Short Analysis
Different optimizers have distinct **optimizer behavior** characteristics. 
**SGD** uses fixed learning rates and may converge slowly. 
**Momentum** adds velocity to gradient updates, helping escape local minima and converge faster. 
**Adam** adapts learning rates per parameter using moving averages of gradients and squared gradients, typically converging faster and more reliably. 
**AdamW** decouples weight decay from gradient updates, improving generalization compared to Adam. The choice of optimizer affects how the **ReLU activation** gradients are processed and how weights are updated. Adam and AdamW generally show better **generalization** and faster convergence on this task, while SGD may require more epochs to reach similar performance.

## 5. Key Takeaway
Adam and AdamW optimizers typically converge faster and achieve better performance than SGD, with adaptive learning rates that adjust per parameter during training.

---

## ğŸ—£ï¸ Ø´Ø±Ø­ Ø¨Ø§Ù„Ø¹Ø§Ù…ÙŠØ©

### Ø¥ÙŠÙ‡ Ù‡Ùˆ Ø§Ù„Ù€ OptimizerØŸ
Ø§Ù„Ù€ Optimizer Ù‡Ùˆ Ø§Ù„Ù„ÙŠ Ø¨ÙŠÙ‚Ø±Ø± Ø¥Ø²Ø§ÙŠ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙŠØªØ¹Ù„Ù… - ÙŠØ¹Ù†ÙŠ Ø¥Ø²Ø§ÙŠ ÙŠØ¹Ø¯Ù‘Ù„ Ø§Ù„Ù€ weights Ø¨ØªØ§Ø¹ØªÙ‡.

### Ø§Ù„Ù€ Optimizers Ø§Ù„Ù„ÙŠ Ø¬Ø±Ø¨Ù†Ø§Ù‡Ø§:

**1. SGD (Stochastic Gradient Descent)**
- Ø²ÙŠ ÙˆØ§Ø­Ø¯ Ø¨ÙŠÙ…Ø´ÙŠ Ø¨Ø®Ø·ÙˆØ§Øª Ø«Ø§Ø¨ØªØ©
- Ø¨Ø·ÙŠØ¡ Ø¨Ø³ Ø¨Ø³ÙŠØ·
- Ù…Ù…ÙƒÙ† ÙŠØªÙˆÙ‡ ÙÙŠ Ø§Ù„Ù…Ù†Ø­Ø¯Ø±Ø§Øª

**2. Momentum**
- Ø²ÙŠ ÙƒÙˆØ±Ø© Ø¨ØªØªØ¯Ø­Ø±Ø¬ - Ø¨ØªØ§Ø®Ø¯ Ø³Ø±Ø¹Ø© Ù…Ø¹ Ø§Ù„ÙˆÙ‚Øª
- Ø£Ø³Ø±Ø¹ Ù…Ù† SGD
- Ø¨ÙŠØ¹Ø¯ÙŠ Ø§Ù„Ù…Ø·Ø¨Ø§Øª Ø§Ù„ØµØºÙŠØ±Ø©

**3. Adam**
- Ø£Ø°ÙƒÙ‰ optimizer - Ø¨ÙŠØ¸Ø¨Ø· Ø§Ù„Ø³Ø±Ø¹Ø© Ù„ÙƒÙ„ weight Ù„ÙˆØ­Ø¯Ù‡
- Ø³Ø±ÙŠØ¹ ÙˆÙ…Ø³ØªÙ‚Ø±
- Ø§Ù„Ø£Ø´Ù‡Ø± ÙˆØ§Ù„Ø£ÙƒØªØ± Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹

**4. AdamW**
- Ø²ÙŠ Adam Ø¨Ø³ Ø¨ÙŠØ¹Ù…Ù„ weight decay Ø£Ø­Ø³Ù†
- Ø¨ÙŠØ­Ø³Ù† Ø§Ù„Ù€ generalization

### Ø§Ù„Ù†ØªÙŠØ¬Ø©ØŸ
Adam Ùˆ AdamW ÙƒØ§Ù†ÙˆØ§ Ø§Ù„Ø£Ø³Ø±Ø¹ ÙˆØ§Ù„Ø£Ø­Ø³Ù†! ğŸ†

### Ø§Ù„Ø®Ù„Ø§ØµØ©
Ø§Ù„Ù€ Optimizer Ø²ÙŠ Ø§Ù„Ø³ÙˆØ§Ù‚ - ÙÙŠ Ø³ÙˆØ§Ù‚ Ø¨Ø·ÙŠØ¡ ÙˆØ­Ø°Ø± (SGD)ØŒ ÙˆÙÙŠ Ø³ÙˆØ§Ù‚ Ø³Ø±ÙŠØ¹ ÙˆØ´Ø§Ø·Ø± (Adam). Ø§Ø®ØªØ§Ø± Ø§Ù„Ù„ÙŠ ÙŠÙ†Ø§Ø³Ø¨ Ù…Ø´Ø±ÙˆØ¹Ùƒ! ğŸš—
