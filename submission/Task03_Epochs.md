# Task 03 â€” Epochs

## 1. Objective
Compare model performance across different numbers of training epochs (5, 10, 20) to understand the relationship between training duration and validation loss.

## 2. Code Used
```python
epoch_settings = [5, 10, 20]
epoch_histories = {}

for e in epoch_settings:
    model = models.clone_model(base_model)
    model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
    epoch_histories[e] = model.fit(
        x_train, y_train,
        epochs=e,
        validation_data=(x_val, y_val),
        verbose=0
    )
```

## 3. Results
Validation loss curves for 5, 10, and 20 epochs are plotted and saved to `results/loss_curves/epoch_comparison.png`. The curves show how validation loss decreases with more training epochs.

## 4. Short Analysis
More epochs generally lead to lower validation loss, as the **Adam optimizer** continues to update weights and reduce the loss function. However, there's a point of diminishing returnsâ€”after a certain number of epochs, the model may start to **overfit** (training loss continues decreasing while validation loss plateaus or increases). The comparison reveals the trade-off between training time and model performance. The **optimizer behavior** (Adam's adaptive learning rates) helps the model converge efficiently across different epoch counts.

## 5. Key Takeaway
More epochs improve performance up to a point, but excessive training can lead to overfitting; finding the optimal epoch count balances performance and generalization.

---

## ğŸ—£ï¸ Ø´Ø±Ø­ Ø¨Ø§Ù„Ø¹Ø§Ù…ÙŠØ©

### Ø¥ÙŠÙ‡ Ù‡ÙŠ Ø§Ù„Ù€ EpochØŸ
Ø§Ù„Ù€ Epoch ÙŠØ¹Ù†ÙŠ Ø¥Ù† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø´Ø§Ù ÙƒÙ„ ØµÙˆØ± Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙƒØ§Ù…Ù„Ø©. Ù„Ùˆ Ù‚Ù„Ù†Ø§ 10 epochs ÙŠØ¹Ù†ÙŠ Ø´Ø§Ù Ø§Ù„Ø¯Ø§ØªØ§ 10 Ù…Ø±Ø§Øª.

### Ø¥ÙŠÙ‡ Ø§Ù„Ù„ÙŠ Ø¬Ø±Ø¨Ù†Ø§Ù‡ØŸ
Ø¯Ø±Ù‘Ø¨Ù†Ø§ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ 3 Ù…Ø±Ø§Øª:
- Ù…Ø±Ø© Ø¨Ù€ 5 epochs
- Ù…Ø±Ø© Ø¨Ù€ 10 epochs  
- Ù…Ø±Ø© Ø¨Ù€ 20 epochs

### Ø¥ÙŠÙ‡ Ø§Ù„Ù„ÙŠ Ø­ØµÙ„ØŸ
- ÙƒÙ„ Ù…Ø§ Ø²ÙˆØ¯Ù†Ø§ Ø§Ù„Ù€ epochsØŒ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§ØªØ­Ø³Ù† (Ø§Ù„Ù€ loss Ù‚Ù„)
- Ø¨Ø³ ÙÙŠ Ù†Ù‚Ø·Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨ÙŠØ¨Ø¯Ø£ ÙŠØ­ÙØ¸ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø¯Ù„ Ù…Ø§ ÙŠÙÙ‡Ù… (overfitting)

### ÙŠØ¹Ù†ÙŠ Ø¥ÙŠÙ‡ OverfittingØŸ
Ø²ÙŠ Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ù„ÙŠ Ø¨ÙŠØ­ÙØ¸ Ø§Ù„Ø§Ù…ØªØ­Ø§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¨Ø¯Ù„ Ù…Ø§ ÙŠÙÙ‡Ù… Ø§Ù„Ù…Ø§Ø¯Ø©. Ø¨ÙŠØ¬ÙŠØ¨ Ø¯Ø±Ø¬Ø§Øª Ø­Ù„ÙˆØ© ÙÙŠ Ø§Ù„Ù„ÙŠ Ø­ÙØ¸Ù‡ØŒ Ø¨Ø³ Ù„Ùˆ Ø¬Ø§Ù„Ù‡ Ø³Ø¤Ø§Ù„ Ø¬Ø¯ÙŠØ¯ Ø¨ÙŠØ¶ÙŠØ¹!

### Ø§Ù„Ø®Ù„Ø§ØµØ©
Ù„Ø§Ø²Ù… Ù†Ù„Ø§Ù‚ÙŠ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù…Ù† Ø§Ù„Ù€ epochs - Ù…Ø´ Ù‚Ù„ÙŠÙ„ Ù‚ÙˆÙŠ (Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…ÙŠØªØ¹Ù„Ù…Ø´) ÙˆÙ…Ø´ ÙƒØªÙŠØ± Ù‚ÙˆÙŠ (ÙŠØ­ÙØ¸ Ø¨Ø¯Ù„ Ù…Ø§ ÙŠÙÙ‡Ù…).
