# Task 05 â€” Dropout

## 1. Objective
Compare model performance with different dropout rates (0.0, 0.1, 0.3) to understand how dropout regularization affects validation loss and prevents overfitting.

## 2. Code Used
```python
dropout_rates = [0.0, 0.1, 0.3]

for d in dropout_rates:
    model = models.Sequential([
        layers.Flatten(input_shape=(28, 28)),
        layers.Dense(128, activation="relu"),
        layers.Dropout(d),
        layers.Dense(10, activation="softmax")
    ])
    model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
    history = model.fit(x_train, y_train, epochs=15, validation_data=(x_val, y_val), verbose=0)
```

## 3. Results
Validation loss curves for dropout rates 0.0, 0.1, and 0.3 are plotted and saved to `results/loss_curves/dropout_comparison.png`. The curves show how different dropout rates affect model generalization.

## 4. Short Analysis
Dropout is a **regularization** technique that randomly sets a fraction of neurons to zero during training, forcing the network to learn redundant representations and preventing **overfitting**. A dropout rate of 0.0 (no dropout) may show lower training loss but higher validation loss due to overfitting. Moderate dropout (0.1) can improve **generalization** by reducing reliance on specific neurons. Higher dropout (0.3) may underfit if too many neurons are disabled. The **ReLU activation** combined with dropout helps the model learn more robust features. Dropout affects the **optimizer behavior** by introducing noise during training, which regularizes the weight updates.

## 5. Key Takeaway
Dropout regularization prevents overfitting by randomly disabling neurons during training, improving generalization, but excessive dropout can lead to underfitting.

---

## ğŸ—£ï¸ Ø´Ø±Ø­ Ø¨Ø§Ù„Ø¹Ø§Ù…ÙŠØ©

### Ø¥ÙŠÙ‡ Ù‡Ùˆ Ø§Ù„Ù€ DropoutØŸ
ØªØ®ÙŠÙ„ Ø¥Ù†Ùƒ ÙÙŠ ÙØ±ÙŠÙ‚ Ø´ØºÙ„ØŒ ÙˆÙƒÙ„ ÙŠÙˆÙ… Ø¨ØªØ·ÙÙŠ Ø±Ø§Ù†Ø¯ÙˆÙ… Ø´ÙˆÙŠØ© Ù…ÙˆØ¸ÙÙŠÙ†. Ø§Ù„Ø¨Ø§Ù‚ÙŠÙŠÙ† Ù„Ø§Ø²Ù… ÙŠØªØ¹Ù„Ù…ÙˆØ§ ÙŠØ´ØªØºÙ„ÙˆØ§ Ù…Ù† ØºÙŠØ±Ù‡Ù…!

### Ø¥Ø²Ø§ÙŠ Ø¨ÙŠØ´ØªØºÙ„ØŸ
- ÙÙŠ ÙƒÙ„ Ù…Ø±Ø© ØªØ¯Ø±ÙŠØ¨ØŒ Ø¨Ù†Ø·ÙÙŠ Ù†Ø³Ø¨Ø© Ù…Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù€ neurons Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹
- Dropout 0.1 = Ø¨Ù†Ø·ÙÙŠ 10% Ù…Ù† Ø§Ù„Ù€ neurons
- Dropout 0.3 = Ø¨Ù†Ø·ÙÙŠ 30% Ù…Ù† Ø§Ù„Ù€ neurons

### Ù„ÙŠÙ‡ Ø¯Ù‡ Ù…ÙÙŠØ¯ØŸ
- Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…Ø¨ÙŠØ¹ØªÙ…Ø¯Ø´ Ø¹Ù„Ù‰ neurons Ù…Ø¹ÙŠÙ†Ø©
- Ø¨ÙŠØªØ¹Ù„Ù… ÙŠÙƒÙˆÙ† Ø£Ù‚ÙˆÙ‰ ÙˆØ£Ø°ÙƒÙ‰
- Ø¨ÙŠÙ…Ù†Ø¹ Ø§Ù„Ù€ overfitting

### Ø¥ÙŠÙ‡ Ø§Ù„Ù„ÙŠ Ø¬Ø±Ø¨Ù†Ø§Ù‡ØŸ
- 0.0 (Ù…ÙÙŠØ´ dropout) - Ù…Ù…ÙƒÙ† ÙŠØ­ØµÙ„ overfitting
- 0.1 (10%) - ØªÙˆØ§Ø²Ù† ÙƒÙˆÙŠØ³
- 0.3 (30%) - Ù…Ù…ÙƒÙ† ÙŠØ¨Ù‚Ù‰ ÙƒØªÙŠØ± Ù‚ÙˆÙŠ

### Ø§Ù„Ø®Ù„Ø§ØµØ©
Ø§Ù„Ù€ Dropout Ø²ÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ØµØ¹Ø¨ - Ø¨ÙŠØ®Ù„ÙŠ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø£Ù‚ÙˆÙ‰ØŒ Ø¨Ø³ Ù„Ùˆ Ø²ÙˆØ¯Ù†Ø§Ù‡ Ù‚ÙˆÙŠ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…Ø´ Ù‡ÙŠØªØ¹Ù„Ù… Ø­Ø§Ø¬Ø©! âš–ï¸
