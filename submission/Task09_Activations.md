# Task 09 â€” Activations

## 1. Objective
Compare different activation functions (tanh, softsign, GELU) to understand how activation choice affects model performance and training dynamics.

## 2. Code Used
```python
activations = ["tanh", "softsign", tf.keras.activations.gelu]

for act in activations:
    model = models.Sequential([
        layers.Flatten(input_shape=(28, 28)),
        layers.Dense(128, activation=act),
        layers.Dense(10, activation="softmax")
    ])
    model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
    model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val), verbose=0)
```

## 3. Results
Models were trained with tanh, softsign, and GELU activation functions. Training and validation metrics were recorded for each activation function to compare their performance.

## 4. Short Analysis
Different **activations** introduce distinct non-linearities that affect gradient flow and model capacity. 
**tanh** outputs values in [-1, 1] and can suffer from vanishing gradients in deep networks. 
**softsign** is similar to tanh but smoother, potentially providing better gradient flow. 
**GELU** (Gaussian Error Linear Unit) is a smooth, non-monotonic activation that often performs well in modern architectures. The choice of activation affects how the **optimizer behavior** (Adam) processes gradientsâ€”some activations provide smoother gradients than others. The **ReLU activation** (used in the base model) is simple and effective, but alternatives like GELU can sometimes improve **generalization** by introducing different non-linear patterns. Activation functions directly impact the model's ability to learn complex patterns and avoid **overfitting**.

## 5. Key Takeaway
Activation functions shape the model's non-linearity and gradient flow; GELU and other modern activations can sometimes outperform traditional choices like ReLU or tanh.

---

## ğŸ—£ï¸ Ø´Ø±Ø­ Ø¨Ø§Ù„Ø¹Ø§Ù…ÙŠØ©

### Ø¥ÙŠÙ‡ Ù‡ÙŠ Ø§Ù„Ù€ Activation FunctionØŸ
Ø§Ù„Ù€ Activation Ù‡ÙŠ Ø§Ù„Ù„ÙŠ Ø¨ØªÙ‚Ø±Ø± Ø§Ù„Ù€ neuron "ÙŠØ´ØªØºÙ„" ÙˆÙ„Ø§ "ÙŠÙØµÙ„". Ù…Ù† ØºÙŠØ±Ù‡Ø§ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù‡ÙŠØ¨Ù‚Ù‰ Ø®Ø· Ù…Ø³ØªÙ‚ÙŠÙ… Ø¨Ø³ÙŠØ·!

### Ø§Ù„Ù€ Activations Ø§Ù„Ù„ÙŠ Ø¬Ø±Ø¨Ù†Ø§Ù‡Ø§:

**1. tanh**
- Ø¨ÙŠØ·Ù„Ø¹ Ù‚ÙŠÙ… Ù…Ù† -1 Ù„Ù€ 1
- Ù‚Ø¯ÙŠÙ… ÙˆÙ…Ø¹Ø±ÙˆÙ
- âŒ Ù…Ù…ÙƒÙ† ÙŠØ¹Ù…Ù„ vanishing gradient (Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙŠØªÙˆÙ‡)

**2. softsign**
- Ø´Ø¨Ù‡ tanh Ø¨Ø³ Ø£Ù†Ø¹Ù…
- Ø¨ÙŠØ·Ù„Ø¹ Ù‚ÙŠÙ… Ù…Ù† -1 Ù„Ù€ 1
- âœ… gradient Ø£Ø­Ø³Ù† Ø´ÙˆÙŠØ©

**3. GELU**
- Ø§Ù„Ø¬Ø¯ÙŠØ¯ ÙˆØ§Ù„ÙØ®Ù… ğŸŒŸ
- Ø¨ÙŠØ³ØªØ®Ø¯Ù… ÙÙŠ GPT Ùˆ BERT
- âœ… smooth ÙˆÙ…Ø±Ù†
- âœ… Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø²

### Ø¥ÙŠÙ‡ Ø§Ù„ÙØ±Ù‚ØŸ
ØªØ®ÙŠÙ„ Ø¥Ù†Ùƒ Ø¨ØªØ³Ø£Ù„ Ø§Ù„Ù€ neuron "ØªØ´ØªØºÙ„ØŸ":
- **ReLU**: "Ø£ÙŠÙˆÙ‡" Ø£Ùˆ "Ù„Ø£" (0 Ø£Ùˆ Ø§Ù„Ø±Ù‚Ù…)
- **tanh**: "Ø£ÙŠÙˆÙ‡ Ø´ÙˆÙŠØ©" Ø£Ùˆ "Ù„Ø£ Ø´ÙˆÙŠØ©" (-1 Ù„Ù€ 1)
- **GELU**: "Ø£ÙŠÙˆÙ‡ Ø¨Ù†Ø³Ø¨Ø© ÙƒØ°Ø§%" (Ø£Ø°ÙƒÙ‰ ÙˆØ£Ù†Ø¹Ù…)

### Ø§Ù„Ø®Ù„Ø§ØµØ©
Ø§Ù„Ù€ Activation Function Ø²ÙŠ Ø´Ø®ØµÙŠØ© Ø§Ù„Ù€ neuron - ÙÙŠ Ø§Ù„Ø­Ø§Ø¯ (ReLU)ØŒ ÙˆØ§Ù„Ù…Ø±Ù† (GELU). Ø§Ø®ØªØ§Ø± Ø§Ù„Ù„ÙŠ ÙŠÙ†Ø§Ø³Ø¨ Ù…Ø´ÙƒÙ„ØªÙƒ! ğŸ­
